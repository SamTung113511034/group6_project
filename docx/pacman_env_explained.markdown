# Pac-Man 環境（PacManEnv）機制詳解

## 什麼是 PacManEnv？

**PacManEnv** 是一個模擬《Pac-Man》遊戲的環境，專為強化學習設計。它就像一個虛擬的遊戲機，AI 可以在裡面控制 Pac-Man，移動、吃豆子、躲鬼魂，並根據遊戲結果（得分、死亡等）得到反饋來學習更好的玩法。這個環境繼承自遊戲核心邏輯的 `Game` 類，並實現了強化學習需要的功能，例如提供遊戲狀態、接受動作、計算獎勵等。

**簡單比喻**：PacManEnv 像一個《Pac-Man》的遊戲機，AI 是玩家，通過看螢幕（狀態）、按方向鍵（動作）、看分數（獎勵）來玩遊戲。環境負責運行遊戲，告訴 AI 現在的情況和結果，幫助 AI 學會怎麼玩得更好。

---

## OpenAI Gym 規範

**OpenAI Gym** 是一個廣泛使用的強化學習框架，提供了標準化的環境接口，讓開發者能輕鬆設計和測試 RL 演算法。PacManEnv 遵循 Gym 的規範，主要包括以下元素：

1. **環境類（Environment）**：
   - 每個環境是一個 Python 類，提供遊戲或任務的模擬。
   - PacManEnv 繼承自 `Game` 類，並實現 Gym 的接口方法（如 `reset`、`step`、`close`）。

2. **動作空間（Action Space）**：
   - 定義 AI 可以執行的動作集合。
   - 在 PacManEnv 中，使用 `gym.spaces.Discrete(4)`，表示 4 個離散動作：上（0）、下（1）、左（2）、右（3）。

3. **觀察空間（Observation Space）**：
   - 定義 AI 可以觀察到的狀態範圍。
   - 在 PacManEnv 中，使用 `gym.spaces.Box`，表示一個形狀為 `(6, 高度, 寬度)` 的三維陣列，值在 `[0, 1]` 之間，代表遊戲狀態（詳見「狀態表示」）。

4. **方法要求**：
   - `reset()`：重置環境，返回初始狀態和資訊字典。
   - `step(action)`：執行一個動作，返回 (下一個狀態, 獎勵, 是否終結, 是否截斷, 資訊字典)。
   - `close()`：清理環境資源。
   - PacManEnv 實現了這些方法，符合 Gym 的標準。

5. **回報結構**：
   - 每次 `step` 返回四元組 `(state, reward, done, info)`：
     - `state`：下一個狀態。
     - `reward`：執行動作後的獎勵。
     - `done`：是否達到終止狀態（例如遊戲結束）。
     - `info`：額外資訊（例如分數、生命數）。

**為什麼用 Gym 規範？**  
Gym 提供了一致的接口，讓不同的 RL 演算法（例如 DQN、PPO）能直接與 PacManEnv 互動，無需修改程式碼。這就像所有遊戲機都有標準的按鈕和螢幕，玩家（AI）可以輕鬆上手。

---

## PacManEnv 的核心組成

PacManEnv 模擬了一個完整的《Pac-Man》遊戲，包括迷宮、Pac-Man、鬼魂、豆子等。以下是它的主要組成部分：

1. **迷宮（Maze）**：
   - 一個由格子組成的地圖，尺寸為 `(MAZE_WIDTH, MAZE_HEIGHT)`（例如 21x21）。
   - 格子類型包括：
     - 路徑（`TILE_PATH`）：Pac-Man 和鬼魂可以移動的格子。
     - 牆壁（`TILE_WALL`）和邊界（`TILE_BOUNDARY`）：不可通過的障礙。
     - 門（`TILE_DOOR`）：鬼魂專用通道。
     - 鬼魂重生點（`TILE_GHOST_SPAWN`）：鬼魂的起始和重生位置。
   - 使用 `Map` 類生成迷宮，隨機種子（`MAZE_SEED`）確保可重現。

2. **Pac-Man**：
   - 玩家控制的角色，初始位置在迷宮中的路徑格子。
   - 可以執行 4 個動作：上、下、左、右。
   - 有生命值（預設 3 條命），被普通鬼魂撞到會失去一條命。

3. **鬼魂（Ghosts）**：
   - 4 隻鬼魂（紅、粉紅、橘、淺藍），每隻有獨特的追逐策略（參見 `ghost.py`）。
   - 有不同狀態：正常（追逐）、可食用（逃跑）、返回重生點、等待。
   - 每隔 2 幀移動一次（`ghost_move_counter`），速度由 `FPS`（每秒幀數）控制。

4. **豆子（Pellets）**：
   - **分數豆（Score Pellets）**：吃掉增加分數。
   - **能量豆（Power Pellets）**：吃掉讓所有鬼魂變為可食用狀態（持續 `EDIBLE_DURATION` 幀）。
   - 總豆子數（`total_pellets`）是分數豆和能量豆的總和。

**簡單比喻**：PacManEnv 像一個迷宮遊樂場，Pac-Man 是主角，鬼魂是追他的「壞蛋」，豆子是獎勵點，迷宮是遊戲地圖。AI 控制 Pac-Man 在遊樂場跑來跑去，學會怎麼吃最多豆子又不被抓。

---

## PacManEnv 的主要方法

PacManEnv 提供了強化學習需要的接口方法，以下是每個方法的詳細解釋：

### 1. 初始化（`__init__`）

- **功能**：設置遊戲環境，準備迷宮、Pac-Man、鬼魂和豆子。
- **參數**：
  - `width` 和 `height`：迷宮尺寸（預設 `MAZE_WIDTH` 和 `MAZE_HEIGHT`）。
  - `seed`：隨機種子，確保遊戲隨機性可重現。
  - `ghost_penalty_weight`：鬼魂距離的懲罰權重（預設 3.0），影響獎勵計算。
- **初始化內容**：
  - 呼叫父類 `Game` 的初始化，設置玩家名稱為 "RL_Agent"。
  - 定義動作空間（4 個動作）和觀察空間（6 通道狀態）。
  - 初始化迷宮、Pac-Man、鬼魂、豆子。
  - 設置追蹤變數，例如分數（`current_score`）、已吃豆子數（`eaten_pellets`）、幀數（`frame_count`）。
- **比喻**：這就像開啟遊戲機，設置好迷宮地圖、角色位置，並準備好記分板。

### 2. 獲取狀態（`_get_state`）

- **功能**：將遊戲狀態轉為一個 AI 能理解的三維陣列。
- **輸出**：形狀為 `(6, height, width)` 的浮點數陣列，每個通道表示：
  1. Pac-Man 位置（通道 0）：Pac-Man 所在格子設為 1。
  2. 能量豆位置（通道 1）：能量豆格子設為 1。
  3. 分數豆位置（通道 2）：分數豆格子設為 1。
  4. 可食用鬼魂位置（通道 3）：可食用鬼魂格子設為 1。
  5. 普通鬼魂位置（通道 4）：普通鬼魂格子設為 1。
  6. 牆壁位置（通道 5）：牆壁和邊界格子設為 1。
- **為什麼這樣設計**：
  - 6 個通道像 6 張地圖，清楚告訴 AI 遊戲中的關鍵元素。
  - 值為 0 或 1，簡單且易於神經網絡處理。
- **比喻**：這像給 AI 一張迷宮的「彩色地圖」，每種顏色代表一種東西（Pac-Man、鬼魂、豆子、牆），讓 AI 知道現在的情況。

### 3. 獲取專家動作（`get_expert_action`）

- **功能**：模擬一個簡單的規則 AI（行為樹），提供「專家」動作作為參考。
- **怎麼做**：
  - 呼叫 Pac-Man 的 `rule_based_ai_move`，根據迷宮、豆子和鬼魂位置決定移動方向。
  - 將移動方向轉為動作編號（0=上, 1=下, 2=左, 3=右）。
  - 如果移動失敗，隨機選擇一個安全方向（不會撞牆或進入禁止區域）。
- **為什麼需要**：
  - 專家動作用於預訓練或引導 RL 代理，讓 AI 模仿好的玩法。
- **比喻**：這像請了一個《Pac-Man》老玩家，告訴 AI 在這個情況下應該往哪走。

### 4. 重置環境（`reset`）

- **功能**：重新開始遊戲，返回初始狀態。
- **參數**：
  - `seed`：隨機種子。
  - `random_spawn_seed`：隨機 Pac-Man 出生點的種子。
  - `random_maze_seed`：隨機迷宮的種子。
- **怎麼做**：
  - 重置迷宮（可隨機生成新迷宮）。
  - 重置 Pac-Man 位置（可隨機選擇路徑格子）。
  - 重置鬼魂、豆子、分數、生命等。
  - 返回初始狀態（`_get_state`）和空資訊字典。
- **符合 Gym 規範**：
  - 返回 `(state, info)`，`state` 是初始狀態，`info` 是附加資訊。
- **比喻**：這像按遊戲機的「重啟」鍵，回到新遊戲的起始畫面。

### 5. 更新遊戲（`update`）

- **功能**：執行一次遊戲邏輯更新，移動 Pac-Man 和鬼魂，檢查碰撞和吃豆子。
- **參數**：
  - `fps`：每秒幀數，控制遊戲速度。
  - `move_pacman`：一個函數，用來移動 Pac-Man。
- **怎麼做**：
  - 執行 `move_pacman` 移動 Pac-Man。
  - 檢查是否吃到分數豆或能量豆（能量豆讓鬼魂變可食用）。
  - 每 2 幀更新鬼魂位置（呼叫鬼魂的 `move` 方法）。
  - 檢查 Pac-Man 與鬼魂的碰撞（呼叫 `_check_collision`）。
  - 如果所有豆子被吃完，遊戲勝利。
- **比喻**：這像遊戲機每秒更新畫面，讓 Pac-Man 和鬼魂動起來，檢查誰撞到誰，誰吃了什麼。

### 6. 檢查碰撞（`_check_collision`）

- **功能**：檢查 Pac-Man 與鬼魂的碰撞，更新遊戲狀態。
- **參數**：
  - `fps`：每秒幀數。
- **怎麼做**：
  - 計算 Pac-Man 與每個鬼魂的像素距離（若小於半個格子大小，視為碰撞）。
  - 如果撞到可食用鬼魂：加分（根據 `GHOST_SCORES`），鬼魂返回重生點。
  - 如果撞到普通鬼魂：Pac-Man 失去一條命，所有鬼魂返回重生點；若無生命，遊戲結束。
- **比喻**：這像裁判檢查 Pac-Man 有沒有被鬼魂抓到，或者有沒有吃掉害怕的鬼魂。

### 7. 執行動作（`step`）

- **功能**：讓 AI 執行一個動作，更新遊戲並返回強化學習五元組。
- **參數**：
  - `action`：動作編號（0=上, 1=下, 2=左, 3=右）。
- **怎麼做**：
  - 根據動作移動 Pac-Man（檢查是否撞牆）。
  - 呼叫 `update` 更新遊戲狀態。
  - 計算獎勵（詳見「獎勵設計」）。
  - 獲取下一個狀態（`_get_state`）。
  - 檢查是否終結（`done`：遊戲結束）。
  - 返回 `(next_state, reward, done, info)`。
- **符合 Gym 規範**：
  - 返回四元組，`info` 包含分數、幀數等資訊。
- **比喻**：這像 AI 按了一次方向鍵，遊戲機更新畫面，然後告訴 AI：你現在在哪（狀態）、得了多少分（獎勵）、遊戲有沒有結束（終結）。

### 8. 關閉環境（`close`）

- **功能**：清理環境資源，結束遊戲。
- **怎麼做**：
  - 呼叫父類的 `end_game` 方法。
  - 印出「環境已關閉」訊息。
- **符合 Gym 規範**：
  - 提供 `close` 方法，用於釋放資源。
- **比喻**：這像關掉遊戲機，結束遊戲。

---

## 狀態表示

PacManEnv 的狀態是一個 **6 通道的三維陣列**，形狀為 `(6, height, width)`，每個通道像一張地圖，記錄遊戲元素的位置：
- **通道 0**：Pac-Man 位置（1 表示 Pac-Man，0 表示無）。
- **通道 1**：能量豆位置。
- **通道 2**：分數豆位置。
- **通道 3**：可食用鬼魂位置。
- **通道 4**：普通鬼魂位置。
- **通道 5**：牆壁和邊界位置。

**為什麼這樣設計？**  
- 6 個通道清楚分隔遊戲元素，讓 AI 容易理解。
- 使用 0 和 1 的二元值，簡化神經網絡的輸入。
- 三維陣列類似遊戲畫面，適合卷積神經網絡（CNN）處理。

**簡單比喻**：狀態像 6 張透明地圖疊在一起，每張地圖標記一種東西（Pac-Man、鬼魂、豆子、牆），AI 看這 6 張地圖就知道遊戲情況。

---

## 獎勵設計

獎勵（`reward`）是 AI 學習的關鍵，告訴它動作的好壞。PacManEnv 的獎勵由多部分組成，鼓勵 AI 吃豆子、躲鬼魂、探索新區域：

1. **基礎獎勵（分數變化）**：
   - 根據分數變化計算：`reward = (current_score - old_score) * 10`。
   - 例如：吃分數豆得 2 分，獎勵 +20；吃能量豆得 50 分，獎勵 +500。
   - **比喻**：AI 每得分就像拿獎金，分數越高獎金越多。

2. **撞牆懲罰**：
   - 如果動作導致撞牆（移動失敗），給予負獎勵：`-1 * (1 + 0.5 * consecutive_wall_collisions)`。
   - 連續撞牆（`consecutive_wall_collisions`）會增加懲罰，鼓勵 AI 避免無效動作。
   - **比喻**：AI 撞牆就像摔一跤，連續撞會更痛，逼它學會看路。

3. **時間懲罰**：
   - 如果遊戲未結束且無正獎勵，給予負獎勵：`-10 * (1 - 0.5 * eaten_pellets/total_pellets)`。
   - 隨著吃掉更多豆子，懲罰減少，鼓勵快速清場。
   - **比喻**：AI 浪費時間就像扣薪水，吃越多豆子扣得越少。

4. **探索獎勵**：
   - 如果 Pac-Man 移動到新格子（未訪問過的格子），獎勵 +50。
   - 使用 `visited_positions` 集合追蹤已訪問格子。
   - **比喻**：AI 像探險家，發現新地方有獎金。

5. **形勢獎勵（Shape Reward）**：
   - 根據 Pac-Man 與鬼魂、豆子的距離計算，鼓勵好的位置：
     - **可食用鬼魂**：遠離可食用鬼魂扣分（`-ghost_penalty_weight * (dist/max_dist) * 0.5`）。
     - **普通鬼魂**：靠近普通鬼魂扣分（`-ghost_penalty_weight / dist`）。
     - **能量豆**：遠離能量豆扣分（`-ghost_penalty_weight * (dist/max_dist) * 0.4`）。
     - **分數豆**：遠離分數豆扣分（`-ghost_penalty_weight * (dist/max_dist) * 0.3`）。
   - 距離用迷宮最大對角線（`max_dist`）正規化，權重由 `ghost_penalty_weight`（預設 3.0）調整。
   - 形勢獎勵是當前形勢與上次形勢的差值（`shape * 0.95 - last_shape`）。
   - **比喻**：AI 像在迷宮玩捉迷藏，靠近好東西（可食用鬼魂、豆子）有獎，靠近壞東西（普通鬼魂）扣分。

6. **勝利獎勵**：
   - 如果吃完所有豆子（分數豆和能量豆），獎勵 +500。
   - **比喻**：AI 通關遊戲就像贏得大獎。

7. **正規化**：
   - 獎勵被裁剪到 `[-500, 500]`，然後除以 500，範圍為 `[-1, 1]`。
   - 這穩定了 RL 訓練，避免數值過大或過小。

**為什麼這樣設計獎勵？**  
- 基礎獎勵鼓勵吃豆子和鬼魂，符合遊戲目標。
- 撞牆和時間懲罰防止 AI 浪費動作。
- 探索獎勵鼓勵 AI 探索迷宮，找到更多豆子。
- 形勢獎勵引導 AI 做出聰明選擇（靠近豆子，遠離危險）。
- 勝利獎勵激勵 AI 完成遊戲。

---

## 與 OpenAI Gym 的整合

PacManEnv 完全遵循 OpenAI Gym 的規範，具體體現在：

1. **標準接口**：
   - `reset` 返回初始狀態和資訊字典。
   - `step` 返回四元組 `(state, reward, done, info)`。
   - `close` 清理資源。
   - 這些方法讓 PacManEnv 能與 Gym 相容的 RL 演算法（例如 Stable-Baselines3、Ray RLlib）無縫整合。

2. **動作和觀察空間**：
   - 動作空間 `Discrete(4)` 清楚定義了 4 個動作。
   - 觀察空間 `Box` 指定了狀態的形狀和範圍，符合 Gym 的要求。

3. **資訊字典**：
   - `step` 返回的 `info` 包含豐富資訊，例如：
     - `frame_count`：當前幀數。
     - `current_score`：當前分數。
     - `eaten_pellets`：已吃豆子數。
     - `total_pellets`：總豆子數。
     - `valid_step`：動作是否有效。
     - `lives_lost`：是否失去生命。
   - 這些資訊幫助除錯和監控 AI 表現。

4. **隨機性控制**：
   - 使用 `seed`、`random_spawn_seed` 和 `random_maze_seed` 控制隨機性，確保實驗可重現，符合 Gym 的最佳實踐。

**簡單比喻**：PacManEnv 像一個標準化的遊戲機，遵循 Gym 的「遊戲規則」，讓任何 AI 都能輕鬆連上來玩，無論是用 DQN、PPO 還是其他演算法。

---

## 總結

**PacManEnv** 是一個模擬《Pac-Man》遊戲的強化學習環境，遵循 **OpenAI Gym** 規範，為 AI 提供了一個學習玩遊戲的平台。它的核心機制包括：

- **組成**：
  - 迷宮：由路徑、牆壁、門、重生點組成。
  - Pac-Man：可執行 4 個動作，有 3 條命。
  - 鬼魂：4 隻，各有獨特策略，狀態包括正常、可食用等。
  - 豆子：分數豆和能量豆，影響分數和鬼魂狀態。

- **主要方法**：
  - `__init__`：初始化遊戲環境。
  - `_get_state`：生成 6 通道狀態陣列。
  - `get_expert_action`：提供規則 AI 的動作參考。
  - `reset`：重置遊戲，返回初始狀態。
  - `update`：更新遊戲邏輯（移動、吃豆子、碰撞）。
  - `_check_collision`：檢查 Pac-Man 與鬼魂的碰撞。
  - `step`：執行動作，返回強化學習四元組。
  - `close`：關閉環境。

- **狀態表示**：
  - 6 通道陣列，記錄 Pac-Man、豆子、鬼魂、牆壁的位置。

- **獎勵設計**：
  - 基礎獎勵：分數變化（吃豆子、鬼魂）。
  - 撞牆懲罰：避免無效動作。
  - 時間懲罰：鼓勵快速行動。
  - 探索獎勵：發現新格子。
  - 形勢獎勵：根據與鬼魂、豆子的距離。
  - 勝利獎勵：通關遊戲。

- **OpenAI Gym 規範**：
  - 提供標準化的動作空間、觀察空間和接口。
  - 返回四元組和資訊字典，支持各種 RL 演算法。
  - 控制隨機性，確保可重現性。

**最終比喻**：PacManEnv 像一個《Pac-Man》的虛擬遊樂場，AI 在裡面控制 Pac-Man，跑迷宮、吃豆子、躲鬼魂。環境像一個聰明的遊戲機，給 AI 地圖（狀態）、分數（獎勵）和結果（終結），還遵循 Gym 的標準，讓 AI 能輕鬆學習變成遊戲高手！