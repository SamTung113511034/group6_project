# Pac-Man DQN 訓練流程（train.py）詳解

## 什麼是 train.py？

**train.py** 是一個 Python 程式，負責訓練 DQN 代理來玩《Pac-Man》。它就像一個「教練」，帶領 AI（學生）反覆玩遊戲，記錄表現（得分、動作），並教它如何改進策略。這個程式包含了收集專家數據、預訓練、正式訓練、儲存結果等步驟，還支援超參數優化，讓 AI 學得更快更好。

**簡單比喻**：train.py 像一個遊戲教練，帶著 AI 學生（DQN 代理）在《Pac-Man》遊戲機前練習，先模仿高手（專家數據），然後自己玩，記錄每場比賽的表現，調整戰術，最終變成頂尖玩家。

---

## 核心概念

在理解 train.py 之前，先介紹幾個關鍵概念：

1. **DQN 代理（DQNAgent）**：
   - AI 的「大腦」，負責看遊戲畫面（狀態）、選擇動作（上、下、左、右）、學習策略。
   - 使用神經網絡預測每個動作的「好壞」（Q 值），並從遊戲獎勵中學習。

2. **Pac-Man 環境（PacManEnv）**：
   - 模擬《Pac-Man》遊戲的虛擬環境，包含迷宮、Pac-Man、鬼魂、豆子等。
   - 提供狀態（遊戲畫面）、接受動作、返回獎勵（得分或懲罰）和是否結束。

3. **專家數據**：
   - 由規則 AI（行為樹）生成的遊戲記錄，包含狀態和動作，幫助 AI 快速學到好策略。
   - 就像給 AI 看高手的遊戲錄影，讓它模仿。

4. **超參數**：
   - 訓練過程的設置，例如學習速度（`lr`）、記憶庫大小（`batch_size`）、探索程度（`sigma`）。
   - 這些參數影響 AI 學得快慢和好壞，像調整教練的教學方法。

5. **TensorBoard**：
   - 一個記錄和可視化工具，記錄 AI 的表現（獎勵、損失、動作比例等），方便檢查進度。
   - 像教練的記分板，隨時顯示學生的成績和問題。

6. **Optuna**：
   - 一個自動調整超參數的工具，試驗不同設置，找到最佳組合。
   - 像教練試驗不同訓練計劃，找出最有效的教法。

**比喻**：train.py 是教練，DQN 代理是學生，Pac-Man 環境是遊戲機，專家數據是高手錄影，超參數是訓練計劃，TensorBoard 是記分板，Optuna 是幫教練挑最佳計劃的助手。

---

## train.py 的主要功能

train.py 包含三個主要函數和一個主程式部分，以下是它們的詳細解釋：

### 1. 收集專家數據（`collect_expert_data`）

- **功能**：模擬一個規則 AI（專家）玩《Pac-Man》，收集它的遊戲記錄（狀態和動作），用於 AI 的預訓練。
- **參數**：
  - `env`：Pac-Man 環境，模擬遊戲。
  - `agent`：DQN 代理，用於儲存遊戲經驗。
  - `num_episodes`：收集多少場遊戲（預設 `EXPERT_EPISODES`）。
  - `max_steps_per_episode`：每場遊戲最多玩多少步（預設 `EXPERT_MAX_STEPS_PER_EPISODE`）。
  - `expert_random_prob`：專家隨機選擇動作的概率（預設 `EXPERT_RANDOM_PROB`），增加數據多樣性。
  - `max_expert_data`：最多收集多少筆數據（預設 `MAX_EXPERT_DATA`）。
- **怎麼做**：
  - 跑多場遊戲（回合），每場從新迷宮開始（隨機出生點）。
  - 每一步：
    - 大多數時候用專家動作（`env.get_expert_action`），模擬高手玩法。
    - 有小概率（`expert_random_prob`）隨機選動作，增加變化。
    - 執行動作，記錄狀態和動作（如果動作有效）。
    - 將經驗存到代理的記憶庫（`agent.store_transition`）。
  - 每場結束後，印出步數和數據量。
  - 停止條件：數據量達上限或跑完指定回合。
- **輸出**：一個列表，包含 (狀態, 動作) 的專家數據，最多 `max_expert_data` 筆。
- **比喻**：這像教練請了一個《Pac-Man》高手玩遊戲，錄下他的每一步（畫面和按鍵），偶爾讓他亂按幾下，收集一堆錄影給 AI 學。

### 2. 訓練 DQN 代理（`train`）

- **功能**：執行完整的 DQN 訓練流程，包括預訓練、遊戲交互、學習、儲存結果。
- **參數**：
  - `trial`：Optuna 試驗對象，用於超參數優化（可選）。
  - `resume`：是否從現有模型繼續訓練。
  - `model_path` 和 `memory_path`：模型和記憶庫的儲存/載入路徑。
  - `episodes`：訓練多少場遊戲。
  - `early_stop_reward`：如果平均獎勵達到這個值，提前停止。
  - `pretrain_episodes`：預訓練的回合數。
  - 超參數：學習率（`lr`）、批量大小（`batch_size`）、目標更新頻率（`target_update_freq`）、噪聲因子（`sigma`）、N 步回報（`n_step`）、折扣因子（`gamma`）、優先回放參數（`alpha`, `beta`, `beta_increment`）、專家概率（`expert_prob_start`, `expert_prob_end`, `expert_prob_decay_steps`）、專家隨機概率（`expert_random_prob`）、最大專家數據量（`max_expert_data`）、鬼魂懲罰權重（`ghost_penalty_weight`）。
- **怎麼做**：
  1. **設置超參數**：
     - 如果用 Optuna，自動試驗不同超參數（例如 `lr` 從 0.0001 到 0.01）。
     - 否則用預設值（來自 `config.py`）。
     - 檢查參數是否合理（例如 `lr > 0`）。
  2. **初始化環境和代理**：
     - 創建 Pac-Man 環境（`PacManEnv`），設置迷宮大小和鬼魂懲罰權重。
     - 創建 DQN 代理（`DQNAgent`），傳入狀態維度（6 通道陣列）、動作數（4）、設備（GPU 或 CPU）等。
     - 如果 `resume=True` 且模型存在，載入之前的模型和記憶庫。
  3. **預訓練（如果不是繼續訓練）**：
     - 收集專家數據（呼叫 `collect_expert_data`）。
     - 用專家數據預訓練代理（`agent.pretrain`），讓 AI 模仿高手，跑 10,000 步。
  4. **正式訓練**：
     - 跑多場遊戲（回合），每場：
       - 重置環境（隨機出生點）。
       - 重置神經網絡的噪聲（Noisy DQN）。
       - 每一步：
         - 以 `expert_prob` 概率用專家動作，否則用 AI 自己的動作（`agent.choose_action`）。
         - 記錄動作次數、Q 值、與鬼魂的距離、遭遇次數（距離 < 2）。
         - 執行動作（`env.step`），得到新狀態、獎勵、是否結束、資訊。
         - 如果動作有效，儲存經驗（`agent.store_transition`）並學習（`agent.learn`）。
         - 記錄損失（`loss`）、獎勵、生命損失等。
       - 每場結束，記錄總獎勵、平均鬼魂距離、遭遇次數、生命損失。
       - 用 TensorBoard 記錄指標（獎勵、損失、Q 值、動作比例等）。
       - 每 5 場儲存模型和記憶庫。
     - 停止條件：跑完指定回合或平均獎勵達 `early_stop_reward`。
  5. **儲存結果**：
     - 儲存最終模型（`pacman_dqn_final.pth`）和記憶庫（`replay_buffer_final.pkl`）。
     - 將所有回合的獎勵存為 JSON 檔案（`episode_rewards.json`）。
  6. **清理**：
     - 關閉 TensorBoard 和環境。
- **輸出**：最終評估分數（最近 100 場的平均獎勵 + 10 * 平均鬼魂距離 - 50 * 平均生命損失）。
- **比喻**：這像教練帶學生練《Pac-Man》，先看高手錄影（預訓練），然後自己玩，反覆記錄表現（獎勵、距離）、調整戰術（學習），每隔幾場存進度，最後看學生成績（評估分數）。

### 3. Optuna 優化目標（`objective`）

- **功能**：定義 Optuna 的超參數優化目標，通過訓練評估不同參數組合。
- **參數**：
  - `trial`：Optuna 試驗對象，負責建議超參數。
- **怎麼做**：
  - 呼叫 `train` 函數，設置 500 回合，早期停止獎勵為 10,000。
  - 返回訓練的評估分數（同 `train` 的輸出）。
- **比喻**：這像教練的助手，試驗不同訓練計劃（超參數），挑出讓學生學得最好的方法。

### 4. 主程式（`if __name__ == "__main__"`）

- **功能**：解析命令行參數，決定是單次訓練還是用 Optuna 優化。
- **怎麼做**：
  - 定義命令行參數（`argparse`），包括：
    - `--resume`：是否繼續訓練。
    - `--optuna`：是否用 Optuna 優化。
    - 訓練設置：回合數、預訓練回合數、早期停止獎勵等。
    - DQN 參數：學習率、批量大小、噪聲因子等。
    - 專家數據參數：回合數、隨機概率等。
  - 如果 `--optuna` 啟用：
    - 創建 Optuna 研究，目標是最大化評估分數。
    - 跑 50 次試驗，印出最佳參數和分數。
  - 否則：
    - 用命令行指定的參數跑單次訓練（呼叫 `train`）。
- **比喻**：這像教練看訓練計劃書（命令行參數），決定是按計劃練（單次訓練）還是試驗多種方法（Optuna 優化）。

---

## 訓練流程詳解

以下是 `train` 函數的訓練流程，用簡單語言分步說明：

1. **準備工作**：
   - 檢查電腦有沒有 GPU（`cuda`），用 GPU 加快訓練，否則用 CPU。
   - 設置超參數（學習速度、記憶庫大小等），用 Optuna 試驗或預設值。
   - 啟動遊戲機（`PacManEnv`），設置迷宮和鬼魂懲罰。
   - 創建 AI 學生（`DQNAgent`），給它遊戲畫面大小（狀態維度）和按鍵數（動作數）。
   - 如果繼續訓練，拿出之前的課本（模型）和筆記（記憶庫）。

2. **預訓練（模仿高手）**：
   - 如果是新學生，收集高手錄影（專家數據）。
   - 讓 AI 看錄影 10,000 次（`pretrain`），學會模仿高手的動作。

3. **正式訓練（自己玩）**：
   - 玩多場遊戲（回合），每場：
     - 開始新遊戲（`env.reset`），隨機出生點。
     - 清空 AI 的「隨機想法」（重置 Noisy DQN 噪聲）。
     - 每一步：
       - 偶爾問高手怎麼走（專家動作），大多數時候自己選（AI 動作）。
       - 記錄按了哪個鍵（動作）、畫面好壞（Q 值）、離鬼魂多遠。
       - 按鍵（`env.step`），看結果（新畫面、獎勵、是否結束）。
       - 如果動作有效，記進筆記（記憶庫）並複習（學習）。
       - 記錄成績（獎勵）、摔跤次數（生命損失）、鬼魂遭遇。
     - 每場結束，寫下總成績（TensorBoard）。
     - 每 5 場存進度（模型和記憶庫）。
   - 如果成績很好（平均獎勵達 `early_stop_reward`），提前畢業。
   - 如果用 Optuna，每 50 場檢查表現，差的計劃直接放棄（剪枝）。

4. **結束訓練**：
   - 存最終課本（模型）和筆記（記憶庫）。
   - 把每場成績存成日記（JSON 檔案）。
   - 關掉記分板（TensorBoard）和遊戲機（環境）。
   - 給 AI 打分（平均獎勵 + 鬼魂距離 - 生命損失）。

**比喻**：這像教練帶學生練遊戲，先模仿高手，然後自己玩，每步記錄表現（得分、離鬼魂距離），定期存進度，成績好就畢業，最後看總成績。

---

## 關鍵技術

train.py 使用了一些進階技術，讓訓練更有效：

1. **專家數據預訓練**：
   - 用規則 AI 的動作初始化 AI，減少初期亂試的時間。
   - **比喻**：讓學生先看高手錄影，學基本技巧。

2. **Noisy DQN**：
   - 在神經網絡中加入隨機噪聲（`sigma`），鼓勵 AI 試新動作。
   - 記錄噪聲指標（`fc1_weight_sigma_mean` 等），檢查探索程度。
   - **比喻**：讓學生偶爾試新戰術，免得總用同一招。

3. **N 步回報（`n_step`）**：
   - 考慮未來幾步的獎勵（`gamma` 折扣），讓 AI 更看重長期效果。
   - **比喻**：讓學生想「這一步走完，後面幾步能拿多少分？」。

4. **優先回放（`alpha`, `beta`）**：
   - 優先複習重要的經驗（錯誤大的），`beta_increment` 調整重要性採樣。
   - **比喻**：讓學生重點複習錯題，而不是每題都看。

5. **TensorBoard 可視化**：
   - 記錄獎勵、損失、Q 值、動作比例、鬼魂距離等，方便檢查 AI 進度。
   - **比喻**：教練的記分板，隨時顯示學生的表現曲線。

6. **Optuna 超參數優化**：
   - 自動試驗不同參數組合（`lr`, `batch_size` 等），找到最佳設置。
   - 剪枝（`trial.should_prune`）停止表現差的試驗，節省時間。
   - **比喻**：教練的助手試驗不同教法，挑出最有效的。

7. **早期停止（`early_stop_reward`）**：
   - 如果 AI 表現很好（平均獎勵高），提前結束訓練。
   - **比喻**：學生考試滿分就不用再練了。

---

## 命令行參數

train.py 支援命令行參數，讓使用者靈活調整訓練設置。以下是主要參數：

- **訓練設置**：
  - `--resume`：繼續之前的訓練（預設 False）。
  - `--optuna`：用 Optuna 優化超參數（預設 False）。
  - `--episodes`：訓練回合數（預設 `TRAIN_EPISODES`）。
  - `--pretrain_episodes`：預訓練回合數（預設 `PRETRAIN_EPISODES`）。
  - `--early_stop_reward`：早期停止獎勵（預設 `EARLY_STOP_REWARD`）。
  - `--model_path` 和 `--memory_path`：模型和記憶庫路徑。

- **DQN 參數**：
  - `--lr`：學習率（預設 `LEARNING_RATE`）。
  - `--batch_size`：批量大小（預設 `BATCH_SIZE`）。
  - `--target_update_freq`：目標更新頻率（預設 `TARGET_UPDATE_FREQ`）。
  - `--sigma`：噪聲因子（預設 `SIGMA`）。
  - `--n_step`：N 步回報（預設 `N_STEP`）。
  - `--gamma`：折扣因子（預設 `GAMMA`）。
  - `--alpha` 和 `--beta`：優先回放參數（預設 `ALPHA`, `BETA`）。
  - `--beta_increment`：beta 增量（預設 `BETA_INCREMENT`）。
  - `--expert_prob_start` 和 `--expert_prob_end`：專家概率範圍（預設 `EXPERT_PROB_START`, `EXPERT_PROB_END`）。
  - `--expert_prob_decay_steps`：專家概率衰減步數（預設 `EXPERT_PROB_DECAY_STEPS`）。
  - `--ghost_penalty_weight`：鬼魂懲罰權重（預設 `GHOST_PENALTY_WEIGHT`）。

- **專家數據參數**：
  - `--expert_episodes`：專家數據回合數（預設 `EXPERT_EPISODES`）。
  - `--expert_max_steps_per_episode`：每回合最大步數（預設 `EXPERT_MAX_STEPS_PER_EPISODE`）。
  - `--expert_random_prob`：專家隨機概率（預設 `EXPERT_RANDOM_PROB`）。
  - `--max_expert_data`：最大數據量（預設 `MAX_EXPERT_DATA`）。

**比喻**：這些參數像教練的訓練計劃書，告訴他練多久、用什麼方法、模仿高手多少次，還能讓他試驗新計劃（Optuna）。

---

## 總結

**train.py** 是《Pac-Man》DQN 訓練的核心程式，負責讓 AI 從新手變成高手。它的主要功能包括：

- **收集專家數據（`collect_expert_data`）**：
  - 模擬規則 AI，記錄狀態和動作，幫助 AI 快速上手。
- **訓練 DQN 代理（`train`）**：
  - 初始化環境和代理，進行預訓練和正式訓練。
  - 記錄表現（TensorBoard），儲存進度，支援早期停止。
- **Optuna 優化（`objective`）**：
  - 自動試驗超參數，找到最佳訓練設置。
- **主程式**：
  - 解析命令行參數，支援單次訓練或 Optuna 優化。

**關鍵技術**：
- 專家數據預訓練：模仿高手。
- Noisy DQN：鼓勵探索。
- N 步回報：考慮長期效果。
- 優先回放：重點學習重要經驗。
- TensorBoard：可視化進度。
- Optuna：優化參數。
- 早期停止：高效訓練。

**最終比喻**：train.py 像一個聰明的遊戲教練，帶著 AI 學生（DQN 代理）在《Pac-Man》遊戲機前練習。先讓學生看高手錄影（專家數據），然後自己玩，記錄每場比賽的成績（獎勵）、離鬼魂的距離、摔跤次數（生命損失）。教練用記分板（TensorBoard）追蹤進度，試驗不同教法（Optuna），存下課本（模型），最後讓學生成為《Pac-Man》高手！