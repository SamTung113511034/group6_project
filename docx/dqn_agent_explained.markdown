# Pac-Man DQN 代理行為機制詳解

## 什麼是 DQN 代理？

**DQN（Deep Q-Network）代理** 是一個用來控制 Pac-Man 的智能程式，相當於 Pac-Man 的「大腦」。它通過觀察遊戲狀態（例如迷宮、鬼魂位置、Pac-Man 位置等），選擇最佳動作（上、下、左、右），並從遊戲的獎勵（例如吃到分數球加分、被鬼魂抓住扣分）中學習如何玩得更好。

**簡單比喻**：DQN 代理就像一個新手玩家，一開始不知道怎麼玩《Pac-Man》，但它會記住每一步的結果（比如「吃了分數球很好，被鬼魂抓住很糟」），慢慢學會聰明的玩法，就像一個人通過練習變成遊戲高手。

---

## DQN 代理的核心概念

DQN 代理的工作基於 **強化學習**，這是一種讓 AI 通過「試錯」學習的方法。以下是它的幾個核心概念：

1. **狀態（State）**：
   - 遊戲的當前情況，例如迷宮地圖、Pac-Man 的位置、鬼魂的位置、分數球和能量球的位置等。
   - 在程式中，狀態是一個多維數組（形狀為 `(通道數, 高度, 寬度)`），像一張遊戲畫面的「數位照片」。

2. **動作（Action）**：
   - Pac-Man 可以做的選擇，例如向上、向下、向左、向右移動。
   - 程式定義了 `action_dim` 個動作（通常是 4 個，對應四個方向）。

3. **獎勵（Reward）**：
   - 遊戲給 AI 的反饋，告訴它動作的好壞。例如：
     - 吃分數球：+分（正獎勵）。
     - 吃能量球：+更多分。
     - 被鬼魂抓住：-分（負獎勵）。
     - 什麼都沒做：小負獎勵（鼓勵行動）。
   - 獎勵是 AI 學習的依據，它試圖選擇能得到最多獎勵的動作。

4. **Q 值（Q-Value）**：
   - Q 值是 AI 對「某個狀態下選擇某個動作有多好」的評分。
   - DQN 使用一個神經網絡（像一個複雜的數學模型）來預測每個動作的 Q 值，然後選擇 Q 值最高的動作。
   - **比喻**：Q 值像一個分數表，告訴 AI 在某個情況下走哪條路最划算。

5. **學習目標**：
   - AI 的目標是通過反覆玩遊戲，調整神經網絡，讓 Q 值預測更準確，最終學會最佳策略（例如躲鬼魂、吃最多分數球）。

**簡單比喻**：想像 AI 是個學生，遊戲是考試，狀態是考題，動作是答題選項，獎勵是分數，Q 值是學生對每個選項的信心。學生一開始亂猜，但通過看分數（獎勵）慢慢學會怎麼答對。

---

## DQN 代理的組成部分

DQN 代理由幾個關鍵部分組成，協同工作讓 AI 能學習和行動：

1. **神經網絡（DQN）**：
   - 這是 AI 的「大腦」，用來預測 Q 值。
   - 有兩個神經網絡：
     - **主模型（model）**：用來選擇動作和學習。
     - **目標模型（target_model）**：用來計算未來的 Q 值，幫助穩定學習。
   - 神經網絡接收狀態（遊戲畫面），輸出每個動作的 Q 值。
   - **比喻**：神經網絡像一個聰明的計算機，根據遊戲畫面猜哪個方向最好走。

2. **回放緩衝區（Replay Buffer）**：
   - 這是一個記憶庫，儲存 AI 的遊戲經驗（狀態、動作、獎勵、下一個狀態、是否結束）。
   - 使用 **SumTree** 結構，根據經驗的「重要性」（優先級）儲存和抽取。
   - **比喻**：回放緩衝區像 AI 的日記本，記錄每次玩遊戲的經歷，方便回顧和學習。

3. **N 步回報（N-Step Returns）**：
   - AI 不只看當前的獎勵，還會考慮未來幾步的累計獎勵（由 `N_STEP` 決定）。
   - 這讓 AI 能更好地評估動作的長期效果。
   - **比喻**：AI 不只看眼前的分數，還會想「如果我走這一步，後面幾步能拿多少分？」。

4. **優先級回放（Prioritized Experience Replay）**：
   - AI 優先學習「重要」的經驗（例如導致高獎勵或大錯誤的經驗），而不是隨機學習。
   - 使用 `alpha` 和 `beta` 參數控制優先級和重要性採樣。
   - **比喻**：AI 像個聰明的學生，會重點複習錯得多的題目，而不是每題都看。

5. **專家數據（Expert Data）**：
   - AI 可以用人類玩家或其他 AI（例如行為樹）的動作進行「預訓練」，模仿它們的行為。
   - 隨著訓練進行，AI 逐漸減少依賴專家（`expert_prob` 從高到低衰減）。
   - **比喻**：AI 一開始像跟著老師學，模仿老師的玩法，後來慢慢自己摸索。

---

## DQN 代理的工作流程

DQN 代理的工作可以分為三個主要階段：**行動**、**儲存經驗**和**學習**。以下是詳細解釋：

### 1. 行動（選擇動作）

方法：`choose_action`

- **怎麼做**：
  - AI 觀察當前遊戲狀態（例如迷宮和鬼魂位置）。
  - 將狀態輸入神經網絡，得到每個動作的 Q 值。
  - 選擇 Q 值最高的動作（例如「向上」）。
  - 使用 **Noisy DQN**，在神經網絡中加入隨機噪聲，鼓勵 AI 探索不同動作，而不是總選同一個。
- **比喻**：AI 像看著遊戲畫面，根據「感覺」挑一個最好的方向走，但有時會故意試試新方向，免得卡在老路上。

### 2. 儲存經驗（記錄遊戲經歷）

方法：`store_transition`

- **怎麼做**：
  - 每次行動後，AI 記錄一組數據：當前狀態、選擇的動作、得到的獎勵、下一個狀態、是否結束（例如被鬼魂抓住）。
  - 這些數據先存到 **N 步回放緩衝區**，等累積了 N 步或遊戲結束，計算 **N 步回報**（未來幾步的累計獎勵）。
  - 然後將數據存到 **優先級回放緩衝區**，並給每條經驗一個「優先級」（重要性），重要的經驗更容易被抽到學習。
- **比喻**：AI 像在寫日記，每次行動記一筆：「我走了右邊，吃了個分數球，得了 10 分，現在到了新位置。」等記了幾筆，總結一下這幾步的總分數，然後存進日記本。

### 3. 學習（更新神經網絡）

方法：`learn`

- **怎麼做**：
  - 從回放緩衝區隨機抽一批經驗（根據優先級，重要的經驗更容易被抽到）。
  - 用主模型計算當前狀態的 Q 值，用目標模型計算下一個狀態的 Q 值。
  - 計算 **TD 誤差**（預測 Q 值和實際獎勵的差距），作為學習的依據。
  - 使用 **Adam 優化器**調整神經網絡參數，讓 Q 值預測更準確。
  - 定期（每 `TARGET_UPDATE_FREQ` 步）更新目標模型，保持穩定學習。
  - 使用 **自動混合精度（AMP）**和 **梯度裁剪**，提高訓練效率並防止數值問題。
- **比喻**：AI 翻開日記本，挑幾頁重要的記錄看，比較自己當初的「猜測」（Q 值）和實際結果（獎勵），然後調整自己的想法，讓下次猜得更準。

---

## 預訓練（模仿專家）

方法：`pretrain`

- **怎麼做**：
  - 在正式訓練前，AI 可以用「專家數據」（例如人類玩家或行為樹的動作）進行預訓練。
  - 專家數據是一堆 (狀態, 動作) 的組合，告訴 AI 在某個情況下應該做什麼。
  - AI 用這些數據訓練神經網絡，試圖模仿專家的選擇（用交叉熵損失計算誤差）。
  - 預訓練跑固定步數（預設 1000 步），每 100 步顯示損失值。
- **為什麼需要**：
  - 預訓練讓 AI 快速學到好的起點，而不是從零開始亂試。
  - **比喻**：AI 像個新手，先看老師的遊戲錄影，學會一些基本技巧，然後再自己上場練習。

---

## 專家策略的衰減

方法：`update_expert_prob`

- **怎麼做**：
  - 在訓練初期，AI 有一定概率（`expert_prob`）直接使用專家動作，而不是自己的神經網絡。
  - 隨著訓練進行（`steps` 增加），專家概率從 `expert_prob_start`（例如 1.0）線性衰減到 `expert_prob_end`（例如 0.1），衰減過程持續 `expert_prob_decay_steps` 步。
- **為什麼需要**：
  - 初期用專家動作讓 AI 學到好行為，後期減少依賴，讓 AI 自己探索。
  - **比喻**：一開始 AI 完全聽老師的，後來慢慢自己做決定，但偶爾還會問問老師。

---

## 儲存和載入模型

- **儲存（`save`）**：
  - 將神經網絡參數存到檔案（`model_path`）。
  - 將回放緩衝區的數據存到檔案（`memory_path`）。
  - **比喻**：AI 把學到的知識（模型）和日記本（經驗）存起來，方便以後用。

- **載入（`load`）**：
  - 從檔案載入神經網絡參數，複製到主模型和目標模型。
  - 可選擇載入回放緩衝區數據，繼續之前的學習。
  - **比喻**：AI 拿出之前的知識和日記本，馬上恢復到之前的水平，繼續學習。

---

## 進階技術

DQN 代理使用了一些進階技術，讓學習更高效、更穩定：

1. **Noisy DQN**：
   - 在神經網絡中加入隨機噪聲（由 `sigma` 控制），鼓勵 AI 探索不同動作，而不是總選 Q 值最高的動作。
   - **比喻**：AI 不會總走「看起來最好」的路，會偶爾試試新路，發現更好的玩法。

2. **優先級回放（Prioritized Experience Replay）**：
   - 使用 `SumTree` 結構儲存經驗，重要的經驗（TD 誤差大的）更容易被抽到。
   - `alpha` 控制優先級的影響程度，`beta` 控制重要性採樣（避免過分偏向重要經驗）。
   - **比喻**：AI 像個聰明的學生，重點複習錯得多的題目，但也會適當看看其他題。

3. **N 步回報**：
   - 考慮未來 N 步的累計獎勵（`N_STEP`），讓 AI 更關注長遠效果。
   - **比喻**：AI 不只看眼前的分數，還會想「這一步走完，後面幾步能拿多少分？」。

4. **Double DQN**：
   - 主模型選擇最佳動作，目標模型計算 Q 值，減少 Q 值過高估計的問題。
   - **比喻**：AI 請了兩個老師，一個挑選答案，一個檢查答案，確保不會太自信。

5. **自動混合精度（AMP）**：
   - 在 GPU 上使用混合精度計算，加快訓練速度，節省記憶體。
   - **比喻**：AI 用更高效的計算方式，就像用更快的筆記本電腦做作業。

6. **梯度裁剪**：
   - 限制神經網絡參數的更新幅度（`max_norm=2.0`），防止學習過程不穩定。
   - **比喻**：AI 調整知識時小心翼翼，避免一次改太多出錯。

---

## 為什麼用 DQN 代理？

DQN 代理結合了深度學習和強化學習，適合像《Pac-Man》這樣複雜的遊戲：
- **強大的學習能力**：神經網絡能處理複雜的遊戲狀態（迷宮、鬼魂等），學會複雜策略。
- **探索與利用**：Noisy DQN 和優先級回放讓 AI 平衡「試新動作」和「用已知好動作」。
- **長期規劃**：N 步回報讓 AI 考慮未來獎勵，做出更有遠見的選擇。
- **穩定性**：Double DQN、目標模型和梯度裁剪讓學習過程更穩定。
- **模仿專家**：預訓練和專家概率讓 AI 快速上手，減少初期亂試的時間。

**簡單比喻**：DQN 代理像一個聰明的學生，一開始模仿老師（專家數據），通過玩遊戲記錄經驗（回放緩衝區），反覆複習（學習），慢慢變成《Pac-Man》高手，還能自己發現新技巧。

---

## 總結

DQN 代理是《Pac-Man》遊戲中一個聰明的 AI，通過以下方式工作：
- **行動**：用神經網絡根據遊戲畫面選擇最佳方向（`choose_action`）。
- **記錄**：把每一步的經驗存進日記本（`store_transition`），重點記重要的經驗。
- **學習**：從日記本中挑選經驗，調整神經網絡，讓選擇更聰明（`learn`）。
- **預訓練**：模仿專家玩法，快速學到好技巧（`pretrain`）。
- **儲存與載入**：保存和恢復知識，方便繼續學習（`save` 和 `load`）。
- **進階技術**：
  - Noisy DQN：鼓勵試新動作。
  - 優先級回放：重點學重要的經驗。
  - N 步回報：考慮長遠獎勵。
  - Double DQN：穩定 Q 值預測。
  - 自動混合精度和梯度裁剪：提高效率和穩定性。

**最終比喻**：DQN 代理像一個從新手變高手的玩家，一開始跟著老師學，記錄每次遊戲的經驗，重點複習關鍵時刻，慢慢學會怎麼躲鬼魂、吃分數球，還能自己發現新玩法。它的大腦（神經網絡）和日記本（回放緩衝區）讓它越來越聰明，最終成為《Pac-Man》的頂尖玩家！